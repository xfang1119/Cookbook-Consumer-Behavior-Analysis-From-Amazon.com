{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenization\n",
    "    tokens = []\n",
    "    text = str(text).lower()\n",
    "\n",
    "    pattern= r'[a-z]+[.''\\'-_]*[a-z]+'\n",
    "    tokensall = nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    for token in tokensall:  \n",
    "        if token not in stop_words:\n",
    "            tokens.append(token)  \n",
    "\n",
    "    return tokens\n",
    "\n",
    "def sentiment_analysis(text, positive_words, negative_words):\n",
    "    # count the number of positive words and negative words\n",
    "    negations=[\"not\", \"no\", \"isn't\", \"wasn't\", \"aren't\", \\\n",
    "               \"weren't\", \"don't\", \"didn't\", \"cannot\", \\\n",
    "               \"couldn't\", \"won't\", \"neither\", \"nor\"]\n",
    "    \n",
    "    sentiment = None\n",
    "    \n",
    "    tokens=tokenize(text)\n",
    "    positive_tokens=[]\n",
    "    negative_tokens=[]\n",
    "    \n",
    "    positive_words=[token for token in tokens if token in positive_words]\n",
    "    negative_words=[token for token in tokens if token in negative_words]\n",
    "    \n",
    "    for idx, token in enumerate(positive_words):\n",
    "        if token in positive_words:\n",
    "            if idx!=0:\n",
    "                if tokens[idx-1] not in negative_words:\n",
    "                    positive_tokens.append(token)\n",
    "                if tokens[idx-1]  in negative_words:\n",
    "                    negative_tokens.append(token)\n",
    "            else:\n",
    "                positive_tokens.append(token)\n",
    "                \n",
    "    for idx, token in enumerate(negative_words):\n",
    "        if token in negative_words:\n",
    "            if idx!=0:\n",
    "                if tokens[idx-1] not in negative_words:\n",
    "                    negative_tokens.append(token)\n",
    "                if tokens[idx-1]  in negative_words:\n",
    "                    positive_tokens.append(token)\n",
    "            else:\n",
    "                negative_tokens.append(token)\n",
    "    \n",
    "    # compare the number and determine the label\n",
    "    if len(positive_tokens)>len(negative_tokens):\n",
    "        sentiment = 1\n",
    "    if len(positive_tokens)<=len(negative_tokens):\n",
    "        sentiment = 0\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def performance_evaluate(input_file, positive_words, negative_words):\n",
    "    # test the accuracy of sentiment analysis model\n",
    "    accuracy = None\n",
    "    data=input_file\n",
    "    label=data['sentiment'].tolist()\n",
    "    review=data['review'].tolist()\n",
    "    corr_data=list(zip(label,review))\n",
    "    \n",
    "    correct_pre = 0\n",
    "\n",
    "    for label, review in corr_data:\n",
    "        pre_sentiment=sentiment_analysis(review, positive_words, negative_words)\n",
    "        if pre_sentiment == label:\n",
    "            correct_pre+=1\n",
    "        \n",
    "    accuracy=correct_pre/len(corr_data)\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster0 sentiment accuracy: 0.70\n",
      "\n",
      "Cluster1 sentiment accuracy: 0.81\n",
      "\n",
      "Cluster2 sentiment accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # cluster0\n",
    "    path =r'C:\\Users\\liulu\\OneDrive\\Documents\\19fall\\BIA660\\FinalPJ\\Reviews\\Reviews\\Cluster0' # use your path\n",
    "    allFiles = glob.glob(path + \"/*.csv\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    cluster0 = pd.concat(list_)\n",
    "\n",
    "    with open(\"positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        \n",
    "    acc=performance_evaluate(cluster0, \\\n",
    "                                  positive_words, negative_words)\n",
    "    print(\"\\nCluster0 sentiment accuracy: {0:.2f}\".format(acc))\n",
    "\n",
    "    # cluster1\n",
    "    path =r'C:\\Users\\liulu\\OneDrive\\Documents\\19fall\\BIA660\\FinalPJ\\Reviews\\Reviews\\Cluster1' # use your path\n",
    "    allFiles = glob.glob(path + \"/*.csv\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    cluster1 = pd.concat(list_)\n",
    "\n",
    "    with open(\"positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        \n",
    "    acc=performance_evaluate(cluster1, \\\n",
    "                                  positive_words, negative_words)\n",
    "    print(\"\\nCluster1 sentiment accuracy: {0:.2f}\".format(acc))    \n",
    "    \n",
    "    # cluster2\n",
    "    path =r'C:\\Users\\liulu\\OneDrive\\Documents\\19fall\\BIA660\\FinalPJ\\Reviews\\Reviews\\Cluster2' # use your path\n",
    "    allFiles = glob.glob(path + \"/*.csv\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    cluster2 = pd.concat(list_)\n",
    "\n",
    "    with open(\"positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        \n",
    "    acc=performance_evaluate(cluster2, \\\n",
    "                                  positive_words, negative_words)\n",
    "    print(\"\\nCluster2 sentiment accuracy: {0:.2f}\".format(acc))    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " sentiment accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # the whole dataset\n",
    "    df = pd.read_csv(r'C:\\Users\\liulu\\OneDrive\\Documents\\19fall\\BIA660\\FinalPJ\\review_join_cluster.csv',index_col=None, header=0)\n",
    "\n",
    "    with open(\"positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        \n",
    "    acc=performance_evaluate(df, \\\n",
    "                                  positive_words, negative_words)\n",
    "    print(\"\\n sentiment accuracy: {0:.2f}\".format(acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

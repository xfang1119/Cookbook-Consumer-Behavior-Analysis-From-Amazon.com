{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The punkt tokenizer is downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shelly/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "import os\n",
    "import glob\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from random import shuffle\n",
    "\n",
    "# download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "print(\"The punkt tokenizer is downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load review data\n",
    "file_path = '/Users/shelly/Google Drive/BIA660/final/Web_Scraping/New_20191128/New/*.csv'\n",
    "addrs = glob.glob(file_path)\n",
    "# load data from Newly scraped results\n",
    "for idx, addr in enumerate(addrs):\n",
    "    if idx == 0:\n",
    "        review_df = pd.read_csv(addr)\n",
    "    else:\n",
    "        new_df = pd.read_csv(addr)\n",
    "        review_df = pd.concat([review_df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5243, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shelly/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# load data from previous clusters\n",
    "dir_list = ['/Users/shelly/Google Drive/BIA660/final/Reviews/Cluster0',\n",
    "            '/Users/shelly/Google Drive/BIA660/final/Reviews/Cluster1-1',\n",
    "            '/Users/shelly/Google Drive/BIA660/final/Reviews/Cluster2']\n",
    "\n",
    "for folder in dir_list:\n",
    "    addrs = glob.glob(folder+'/*.csv')\n",
    "    for addr in addrs:\n",
    "        new_df = pd.read_csv(addr).head(50)\n",
    "        review_df = pd.concat([review_df, new_df], ignore_index=True)\n",
    "del review_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6143, 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster results\n",
    "cluster_df = pd.read_excel('/Users/shelly/Google Drive/BIA660/final/Cluster_Result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(review_df, cluster_df, left_on='book_title', right_on='title')\n",
    "# replace nan in the review field with ''\n",
    "df['review'] = df['review'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a csv\n",
    "df.to_csv('review_join_cluster_6000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('review_join_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>format</th>\n",
       "      <th>helpful</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>star_x</th>\n",
       "      <th>title_x</th>\n",
       "      <th>index</th>\n",
       "      <th>No.</th>\n",
       "      <th>book_link</th>\n",
       "      <th>...</th>\n",
       "      <th>star_y</th>\n",
       "      <th>review_amount</th>\n",
       "      <th>kindle_price</th>\n",
       "      <th>paperback_price</th>\n",
       "      <th>hardcover_price</th>\n",
       "      <th>intro</th>\n",
       "      <th>book_rank</th>\n",
       "      <th>product_desc</th>\n",
       "      <th>abt_author</th>\n",
       "      <th>key_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1615</td>\n",
       "      <td>1536</td>\n",
       "      <td>1219</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>...</td>\n",
       "      <td>1615</td>\n",
       "      <td>1615</td>\n",
       "      <td>1453</td>\n",
       "      <td>1235</td>\n",
       "      <td>442</td>\n",
       "      <td>1585</td>\n",
       "      <td>1615</td>\n",
       "      <td>1019</td>\n",
       "      <td>548</td>\n",
       "      <td>1615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2140</td>\n",
       "      <td>2114</td>\n",
       "      <td>1694</td>\n",
       "      <td>2140</td>\n",
       "      <td>2140</td>\n",
       "      <td>2124</td>\n",
       "      <td>2124</td>\n",
       "      <td>2140</td>\n",
       "      <td>2140</td>\n",
       "      <td>2140</td>\n",
       "      <td>...</td>\n",
       "      <td>2140</td>\n",
       "      <td>2140</td>\n",
       "      <td>2080</td>\n",
       "      <td>1840</td>\n",
       "      <td>1397</td>\n",
       "      <td>2140</td>\n",
       "      <td>1919</td>\n",
       "      <td>1359</td>\n",
       "      <td>441</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2298</td>\n",
       "      <td>2274</td>\n",
       "      <td>1542</td>\n",
       "      <td>2298</td>\n",
       "      <td>2298</td>\n",
       "      <td>2298</td>\n",
       "      <td>2297</td>\n",
       "      <td>2298</td>\n",
       "      <td>2298</td>\n",
       "      <td>2298</td>\n",
       "      <td>...</td>\n",
       "      <td>2298</td>\n",
       "      <td>2298</td>\n",
       "      <td>2088</td>\n",
       "      <td>1415</td>\n",
       "      <td>1682</td>\n",
       "      <td>2298</td>\n",
       "      <td>1785</td>\n",
       "      <td>1735</td>\n",
       "      <td>147</td>\n",
       "      <td>2298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         book_title  format  helpful  review  sentiment  star_x  title_x  \\\n",
       "cluster                                                                    \n",
       "0              1615    1536     1219    1615       1615    1615     1615   \n",
       "1              2140    2114     1694    2140       2140    2124     2124   \n",
       "2              2298    2274     1542    2298       2298    2298     2297   \n",
       "\n",
       "         index   No.  book_link  ...  star_y  review_amount  kindle_price  \\\n",
       "cluster                          ...                                        \n",
       "0         1615  1615       1615  ...    1615           1615          1453   \n",
       "1         2140  2140       2140  ...    2140           2140          2080   \n",
       "2         2298  2298       2298  ...    2298           2298          2088   \n",
       "\n",
       "         paperback_price  hardcover_price  intro  book_rank  product_desc  \\\n",
       "cluster                                                                     \n",
       "0                   1235              442   1585       1615          1019   \n",
       "1                   1840             1397   2140       1919          1359   \n",
       "2                   1415             1682   2298       1785          1735   \n",
       "\n",
       "         abt_author  key_words  \n",
       "cluster                         \n",
       "0               548       1615  \n",
       "1               441       2140  \n",
       "2               147       2298  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shelly/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The punkt tokenizer is downloaded\n"
     ]
    }
   ],
   "source": [
    "# import natural language toolkit\n",
    "import nltk\n",
    "# download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "print(\"The punkt tokenizer is downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Received a 20 year old book. Not the book pictured'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['cluster']==1]['review'])[1241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Corpus contains 1,627,166 characters\n"
     ]
    }
   ],
   "source": [
    "cluster = 1\n",
    "raw_corpus = u''.join(df[df['cluster']==1]['review']+\" \")\n",
    "print(\"Raw Corpus contains {0:,} characters\".format(len(raw_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The punkt tokenizer is loaded\n",
      "We have 18,017 raw sentences\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "print(\"The punkt tokenizer is loaded\")\n",
    "# we tokenize the raw string into raw sentences\n",
    "raw_sentences = tokenizer.tokenize(raw_corpus)\n",
    "print(\"We have {0:,} raw sentences\".format(len(raw_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and split sentence into words\n",
    "def clean_and_split_str(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z]+\")\n",
    "    string = re.sub(strip_special_chars, \" \", string)\n",
    "    return string.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 18,017 clean sentences\n"
     ]
    }
   ],
   "source": [
    "# clean each raw sentences and build the list of sentences\n",
    "sentences = []\n",
    "for raw_sent in raw_sentences:\n",
    "    if len(raw_sent) > 0:\n",
    "        sentences.append(clean_and_split_str(raw_sent))\n",
    "print(\"We have {0:,} clean sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While the compliments are wonderful on my new appearance and it's exciting to be 2 sizes smaller, the real changes (I hoped) were happening on the inside.\n",
      "\n",
      "['While', 'the', 'compliments', 'are', 'wonderful', 'on', 'my', 'new', 'appearance', 'and', 'it', 's', 'exciting', 'to', 'be', 'sizes', 'smaller', 'the', 'real', 'changes', 'I', 'hoped', 'were', 'happening', 'on', 'the', 'inside']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[10])\n",
    "print()\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset corpus contains 300,598 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The dataset corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.word2vec.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=1,\n",
    "    #Seed for the RNG, to make the result reproducible\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    #Number of threads to run in parallel\n",
    "    size=300,\n",
    "    #Dimensionality of the resulting word vectors\n",
    "    min_count=3,\n",
    "    #Minimum word count threshold\n",
    "    window=7)\n",
    "    #Context window length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is built\n"
     ]
    }
   ],
   "source": [
    "word2vec_model.build_vocab(sentences=sentences)\n",
    "print(\"The vocabulary is built\")\n",
    "# print(\"Word2Vec vocabulary length: \", len(list(word2vec_model.vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215889, 300598)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Start training the model\n",
    "for epoch in range(30):\n",
    "    # shuffle the documents in each epoch\n",
    "    shuffle(sentences)\n",
    "word2vec_model.train(sentences=sentences,\n",
    "                     total_examples=word2vec_model.corpus_count,\n",
    "                     epochs=1)\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "word2vec_model.save(\"result2/word2vec_model_trained_on_cluster0.w2v\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_word_2_vec(cluster):\n",
    "    # Convert all the review text into a long string and print its length\n",
    "    raw_corpus = u''.join(df[df['cluster']==cluster]['review']+\" \")\n",
    "    print(\"Raw Corpus contains {0:,} characters\".format(len(raw_corpus)))\n",
    "    # Load the punkt tokenizer\n",
    "    tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "    print(\"The punkt tokenizer is loaded\")\n",
    "    # we tokenize the raw string into raw sentences\n",
    "    raw_sentences = tokenizer.tokenize(raw_corpus)\n",
    "    print(\"We have {0:,} raw sentences\".format(len(raw_sentences)))\n",
    "    \n",
    "    # Clean and split sentence into words\n",
    "    def clean_and_split_str(string):\n",
    "        strip_special_chars = re.compile(\"[^A-Za-z]+\")\n",
    "        string = re.sub(strip_special_chars, \" \", string)\n",
    "        return string.strip().split()\n",
    "    # clean each raw sentences and build the list of sentences\n",
    "    sentences = []\n",
    "    for raw_sent in raw_sentences:\n",
    "        if len(raw_sent) > 0:\n",
    "            sentences.append(clean_and_split_str(raw_sent))\n",
    "    print(\"We have {0:,} clean sentences\".format(len(sentences)))\n",
    "    \n",
    "    token_count = sum([len(sentence) for sentence in sentences])\n",
    "    print(\"The dataset corpus contains {0:,} tokens\".format(token_count))\n",
    "    \n",
    "    word2vec_model = gensim.models.word2vec.Word2Vec(\n",
    "                                                    sg=1,\n",
    "                                                    seed=1,\n",
    "                                                    #Seed for the RNG, to make the result reproducible\n",
    "                                                    workers=multiprocessing.cpu_count(),\n",
    "                                                    #Number of threads to run in parallel\n",
    "                                                    size=300,\n",
    "                                                    #Dimensionality of the resulting word vectors\n",
    "                                                    min_count=3,\n",
    "                                                    #Minimum word count threshold\n",
    "                                                    window=7)\n",
    "                                                    #Context window length\n",
    "    word2vec_model.build_vocab(sentences=sentences)\n",
    "    print(\"The vocabulary is built\")\n",
    "    for epoch in range(30):\n",
    "    # shuffle the documents in each epoch\n",
    "        shuffle(sentences)\n",
    "        word2vec_model.train(sentences=sentences,\n",
    "                             total_examples=word2vec_model.corpus_count,\n",
    "                             epochs=1)\n",
    "    print(\"Training finished\")\n",
    "    word2vec_model.save(\"./results/word2vec_model_trained_on_cluster\"+str(cluster)+\".w2v\")\n",
    "    print(\"Model saved\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Corpus contains 965,846 characters\n",
      "The punkt tokenizer is loaded\n",
      "We have 11,229 raw sentences\n",
      "We have 11,229 clean sentences\n",
      "The dataset corpus contains 179,511 tokens\n",
      "The vocabulary is built\n",
      "Training finished\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "my_word_2_vec(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Corpus contains 1,627,166 characters\n",
      "The punkt tokenizer is loaded\n",
      "We have 18,017 raw sentences\n",
      "We have 18,017 clean sentences\n",
      "The dataset corpus contains 300,598 tokens\n",
      "The vocabulary is built\n",
      "Training finished\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "my_word_2_vec(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Corpus contains 1,465,886 characters\n",
      "The punkt tokenizer is loaded\n",
      "We have 16,172 raw sentences\n",
      "We have 16,172 clean sentences\n",
      "The dataset corpus contains 270,775 tokens\n",
      "The vocabulary is built\n",
      "Training finished\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "my_word_2_vec(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
